---
---

@string{aps = {American Physical Society,}}

@unpublished{ho2024,
  author = {Chung-En Ho, Chung-Ting Tsai, I-Ching Tseng, Chung-Wei Lin},
  title = {A Quantization Strategy for Communication-Efficient DNN Inference in Vehicular Edge Computing},
  note = {Submitted to IEEE Embedded Systems Letters (ESL)},
  abstract = {As Deep Neural Network (DNN) applications drive the development of intelligent connected vehicles (ICV), there is a clear need for efficient and reliable DNN processing in ICVs. Vehicular Edge Computing (VEC) is a promising paradigm to meet such demand, where vehicles can cooperate to perform computation tasks. However, there is currently a mismatch between vehicles’ communication capacities and computation capacities, making the communication channel a scarce resource for DNN inference workloads. This paper proposes a design-time deep reinforcement learning (DRL)-based quantization strategy to alleviate this issue under dynamic communication and computing environments in VEC while complying with a pre-defined accuracy drop constraint. Compared to greedy strategies, our results show significant improvements in communication traffic and inference latency.}, 
  year = {2025}
}

@INPROCEEDINGS{10413767,
  author={Burr, G. W. and Tsai, H. and Simon, W. and Boybat, I. and Ambrogio, S. and Ho, C.-E. and Liou, Z.-W. and Rasch, M. and Büchel, J. and Narayanan, P. and Gordon, T. and Jain, S. and Levin, T. M. and Hosokawa, K. and Le Gallo, M. and Smith, H. and Ishii, M. and Kohda, Y. and Chen, A. and Mackin, C. and Fasoli, A. and ElMaghraoui, K. and Muralidhar, R. and Okazaki, A. and Chen, C. -T. and Frank, M. M. and Lammie, C. and Vasilopoulos, A. and Friz, A. M. and Luquin, J. and Teehan, S. and Ahsan, I. and Sebastian, A. and Narayanan, V.},
  booktitle={2023 International Electron Devices Meeting (IEDM)}, 
  title={Design of Analog-AI Hardware Accelerators for Transformer-based Language Models (Invited)}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  keywords={Training;Semiconductor device modeling;Nonvolatile memory;Computational modeling;Transformers;Throughput;Energy efficiency;In-memory computing;Non-volatile memory;large language models;analog multiply-accumulate for DNN inference;analog AI;deep learning accelerator;system modeling},
  abstract={Analog Non-Volatile Memory-based accelerators offer high-throughput and energy-efficient Multiply-Accumulate operations for the large Fully-Connected layers that dominate Transformer-based Large Language Models. We describe architectural, wafer-scale testing, chip-demo, and hardware-aware training efforts towards such accelerators, and quantify the unique raw-throughput and latency benefits of Fully- (rather than Partially-)Weight-Stationary systems.},
  doi={10.1109/IEDM45741.2023.10413767}
}
